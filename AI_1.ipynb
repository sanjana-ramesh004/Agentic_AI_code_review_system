{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b430a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mistralai python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f458cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from mistralai import Mistral\n",
    "\n",
    "client=Mistral(api_key=os.getenv(\"MISTRAL_API_KEY\"))\n",
    "\n",
    "ml_agent=client.beta.agents.create(\n",
    "    model=\"mistral-medium-2505\",\n",
    "    name=\"ml_agent\",\n",
    "    description=\"Machine learning assistant\",\n",
    "    instructions=\"You're an ML expert. Give practicle, actionable advice.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cc38bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent created: ag_01981f0822d674dfaaac2b056963f042\n"
     ]
    }
   ],
   "source": [
    "print(f\"Agent created: {ml_agent.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c047ad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=client.beta.conversations.start(\n",
    "    agent_id=ml_agent.id,\n",
    "    inputs=\"Should I use Random forest or XGBoost for a 5000 sample dataset? Answer in 3 sentences only.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c12b8e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mistralai.models.conversationresponse.ConversationResponse"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "593b38c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MessageOutputEntry(content=\"For a dataset of 5000 samples, XGBoost is generally a better choice due to its superior performance and efficiency. It often provides better accuracy and handles various types of data well. However, if interpretability is a key concern, Random Forest might be more suitable as it's easier to understand and visualize.\", object='entry', type='message.output', created_at=datetime.datetime(2025, 7, 18, 19, 45, 52, 970041, tzinfo=TzInfo(UTC)), completed_at=datetime.datetime(2025, 7, 18, 19, 45, 54, 339345, tzinfo=TzInfo(UTC)), id='msg_01981f1259497066b5a0bf9c922f141c', agent_id='ag_01981f0822d674dfaaac2b056963f042', model='mistral-medium-2505', role='assistant')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c28c762d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a dataset of 5000 samples, XGBoost is generally a better choice due to its superior performance and efficiency. It often provides better accuracy and handles various types of data well. However, if interpretability is a key concern, Random Forest might be more suitable as it's easier to understand and visualize.\n"
     ]
    }
   ],
   "source": [
    "print(response.outputs[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99bd3d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data agent: ag_019824095d7d7488985a4901557f1e18\n"
     ]
    }
   ],
   "source": [
    "data_agent=client.beta.agents.create(\n",
    "    model=\"mistral-medium-latest\",\n",
    "    name=\"data-expert\",\n",
    "    description=\"Data preprocessing specialist\",\n",
    "    instructions=\"Expert in data cleaning and feature engineering\",\n",
    "    completion_args={\"temperature\":0.1},\n",
    ")\n",
    "print(f\"Data agent: {data_agent.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59be1f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here are three best practices for encoding categorical variables in machine learning:\n",
      "\n",
      "1. **One-Hot Encoding**:\n",
      "   - **Actionable Advice**: Use one-hot encoding for categorical variables with a small number of unique values. This method creates a binary column for each category and returns a vector with a 1 in the position corresponding to the category.\n",
      "   - **Implementation**: Libraries like pandas in Python have built-in functions (e.g., `pd.get_dummies()`) to easily apply one-hot encoding.\n",
      "   - **Consideration**: Be mindful of the \"curse of dimensionality\" if the categorical variable has many unique values, as this can lead to a high-dimensional feature space.\n",
      "\n",
      "2. **Label Encoding**:\n",
      "   - **Actionable Advice**: Use label encoding for ordinal categorical variables where there is a meaningful order between the categories. This method assigns a unique integer to each category.\n",
      "   - **Implementation**: Libraries like scikit-learn provide `LabelEncoder` for this purpose.\n",
      "   - **Consideration**: Avoid using label encoding for nominal categorical variables (where there is no inherent order) as it can mislead the model into thinking there is an ordinal relationship.\n",
      "\n",
      "3. **Embedding for High-Cardinality Categorical Variables**:\n",
      "   - **Actionable Advice**: For categorical variables with high cardinality (many unique values), consider using embeddings. This technique maps each category to a dense vector in a lower-dimensional space.\n",
      "   - **Implementation**: Deep learning frameworks like TensorFlow or PyTorch can be used to create and train embedding layers.\n",
      "   - **Consideration**: Embeddings are particularly useful in neural networks and can capture complex relationships between categories.\n",
      "\n",
      "By following these practices, you can effectively encode categorical variables and improve the performance of your machine learning models.\n"
     ]
    }
   ],
   "source": [
    "response=client.beta.conversations.start(\n",
    "    agent_id=ml_agent.id,\n",
    "    inputs=\"What are the best practices for encoding categorical variables in machine learning? List three\",\n",
    "\n",
    ")\n",
    "print(response.outputs[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c30c91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_019828d58d917752bccd5321124880ca\n"
     ]
    }
   ],
   "source": [
    "print(response.conversation_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e2a97d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your first question was:\n",
      "\n",
      "\"What are the best practices for encoding categorical variables in machine learning? List three\"\n"
     ]
    }
   ],
   "source": [
    "follow_up=client.beta.conversations.append(\n",
    "    conversation_id=response.conversation_id,\n",
    "    inputs=\"What was my first question?\",\n",
    ")\n",
    "print(follow_up.outputs[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99a1071c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You asked for best practices for encoding categorical variables in machine learning, and I provided three actionable methods: one-hot encoding, label encoding, and embeddings for high-cardinality variables. Then, you asked what your first question was, and I repeated it for you. This summarizes our brief conversation so far.\n"
     ]
    }
   ],
   "source": [
    "follow_up=client.beta.conversations.append(\n",
    "    conversation_id=response.conversation_id,\n",
    "    inputs=\"Summarize the entire coo=nverstion in 3 sentences\",\n",
    ")\n",
    "print(follow_up.outputs[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "128134f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mistralai.utils.eventstreaming.EventStream"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message=\"Explain gradient descent in simple terms in 5 sentences\"\n",
    "\n",
    "response=client.beta.conversations.start_stream(\n",
    "    agent_id=ml_agent.id, inputs=message,\n",
    ")\n",
    "\n",
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3a31a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure , here 's a simple explanation  of gradient descent:\n",
      "\n",
      " 1 . **Objective **: Gradient  descent is an  optimization algorithm used to  minimize  some  function by  iteratively moving  toward  the lowest  value  of that  function.\n",
      "2.  **How  it works**: Imagine  you're on  a mountain  and  want  to get  to the lowest  point. You look  around  to  see  which  way  is steep est downward  and  take  a  step in  that direction.\n",
      "3 . **Iter ative  process **: You repeat  this process,  each  time checking  the steep ness  and  taking  another  step downward ,  until you can 't go  any lower .\n",
      " 4. **Learning  rate**: The size  of each  step is  determined  by the learning rate . If  the  steps  are too big , you might overs hoot the lowest point .  If they 're  too small, it  might  take  a  long  time to reach  the bottom .\n",
      "5. ** Usage  in  ML **: In  machine learning, this  process  is used  to update  the  parameters  of a model to  minimize  the error  or  loss function,  making  the  model more accurate .\n",
      "\n",
      " In  essence , gradient descent is  like  carefully  walking  downhill  blind fold ed by  feeling  the slope  beneath  your  feet to  find  the bottom  of  the valley . "
     ]
    }
   ],
   "source": [
    "with response as event_stream:\n",
    "    for event in event_stream:\n",
    "        if event.event==\"message.output.delta\":\n",
    "            print(event.data.content, flush=True, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e46af8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_conversation_id=client.beta.conversations.list()[0].id\n",
    "\n",
    "message=\"Translate your last response to German.\"\n",
    "follow_up=client.beta.conversations.append_stream(\n",
    "    conversation_id=last_conversation_id, inputs=message\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "836af700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hier  ist die  Übersetzung  meiner  letzten Antwort ins  Deutsche :\n",
      "\n",
      "1 . **Ziel **: Der Gradient en ab stieg  ist ein Optim ierungsalgorithmus , der  verwendet  wird , um eine  Funktion  zu minimieren,  indem iter ativ  auf  den  nied rigsten Wert dieser  Funktion zu gegangen  wird .\n",
      "2. ** F unkt ionsweise **: Stellen  Sie sich  vor, Sie  befinden  sich auf einem  Berg  und möchten den  tief sten Punkt erreichen . Sie sch auen sich um , um  zu  sehen , welcher  Weg am  ste ilsten ab wärts  führt ,  und machen  einen  Schritt in diese  Richtung.\n",
      "3.  **Iterat iver Prozess**: Sie  wiederhol en diesen  Prozess, jedes  Mal über prüfen Sie die  Steil heit und machen einen  weiteren  Schritt nach  unten, bis Sie  nicht mehr tiefer  gehen  können.\n",
      "4.  **L ernrate**: Die  Größe jedes  Sch rit ts wird durch die  Lernrate bestimmt .  Wenn die Schritte  zu groß sind,  könnten Sie den  tiefsten Punkt übersch reiten. Wenn  sie zu klein sind , könnte  es lange  dauern, bis  Sie den tiefsten  Punkt erreichen .\n",
      "5. ** Ver wendung im  ML**: Im  mas chinellen Lernen  wird  dieser  Prozess verwendet, um  die Parameter  eines  Mod ells zu aktual isieren, um die  Fehler-  oder Verlust funktion zu minim ieren, wodurch  das  Modell gen auer wird .\n",
      "\n",
      "Z usammengefasst ist  der  Gradientenabstieg  wie  das  vors ichtige Berg ab gehen  mit  verb undenen Augen ,  indem  man  die Ne igung unter  den  Fü ßen spür t, um den  tiefsten Punkt des  T als  zu finden . "
     ]
    }
   ],
   "source": [
    "with follow_up as event_stream:\n",
    "    for event in event_stream:\n",
    "        if event.event==\"message.output.delta\":\n",
    "            print(event.data.content, flush=True, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae8a4423",
   "metadata": {},
   "outputs": [],
   "source": [
    "research_agent=client.beta.agents.create(\n",
    "    model=\"mistral-medium-latest\",\n",
    "    name=\"ml-research-agent\",\n",
    "    description=\"ML research specialist\",\n",
    "    tools=[{\"type\": \"web_search\"}]\n",
    ")\n",
    "\n",
    "search_response=client.beta.conversations.start(\n",
    "    agent_id=research_agent.id, inputs=\"Latest transformer improvemnts in 2025 summarized\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "192295fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextChunk(text='The latest improvements in transformer architecture in 2025 include several key advancements aimed at enhancing performance, efficiency, and scalability. Here are some of the notable developments:  \\n  \\n1. **Pre-normalization**: Modern transformer architectures have shifted from post-normalization to pre-normalization. In pre-normalization, normalization layers are applied before the self-attention and feedforward components. This change improves gradient flow, especially in deep networks, leading to better training stability', type='text'), ToolReferenceChunk(tool='web_search', title='The Evolution of Transformer Architecture: From 2017 to 2024 | by arghya mukherjee | Medium', type='tool_reference', url='https://medium.com/@arghya05/the-evolution-of-transformer-architecture-from-2017-to-2024-5a967488e63b', favicon='https://imgs.search.brave.com/4R4hFITz_F_be0roUiWbTZKhsywr3fnLTMTkFL5HFow/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOTZhYmQ1N2Q4/NDg4ZDcyODIyMDZi/MzFmOWNhNjE3Y2E4/Y2YzMThjNjljNDIx/ZjllZmNhYTcwODhl/YTcwNDEzYy9tZWRp/dW0uY29tLw', description='<strong>Rotary position embeddings (RoPE) replace the sinusoidal positional encodings of the original Transformer</strong>. ... Rotary embeddings encode relative positional information in a way that is more naturally integrated into the self-attention mechanism. By rotating the Query and Key vectors in a shared ...'), TextChunk(text='.  \\n  \\n2. **Rotary Position Embeddings (RoPE)**: Rotary position embeddings have replaced the traditional sinusoidal positional encodings. RoPE encodes relative positional information more effectively, integrating it naturally into the self-attention mechanism. This allows the model to generalize better to sequences longer than those seen during training, improving performance on tasks involving long-context inputs', type='text'), ToolReferenceChunk(tool='web_search', title='The Evolution of Transformer Architecture: From 2017 to 2024 | by arghya mukherjee | Medium', type='tool_reference', url='https://medium.com/@arghya05/the-evolution-of-transformer-architecture-from-2017-to-2024-5a967488e63b', favicon='https://imgs.search.brave.com/4R4hFITz_F_be0roUiWbTZKhsywr3fnLTMTkFL5HFow/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOTZhYmQ1N2Q4/NDg4ZDcyODIyMDZi/MzFmOWNhNjE3Y2E4/Y2YzMThjNjljNDIx/ZjllZmNhYTcwODhl/YTcwNDEzYy9tZWRp/dW0uY29tLw', description='<strong>Rotary position embeddings (RoPE) replace the sinusoidal positional encodings of the original Transformer</strong>. ... Rotary embeddings encode relative positional information in a way that is more naturally integrated into the self-attention mechanism. By rotating the Query and Key vectors in a shared ...'), TextChunk(text='.  \\n  \\n3. **Efficiency Improvements**: Various \"X-former\" models, such as Reformer, Linformer, Performer, and Longformer, have been introduced to improve computational and memory efficiency. These models address the quadratic complexity issue of the self-attention mechanism and reduce computation costs through techniques like pooling and sparsity', type='text'), ToolReferenceChunk(tool='web_search', title='Efficient Transformers: A Survey | ACM Computing Surveys', type='tool_reference', url='https://dl.acm.org/doi/10.1145/3530811', favicon='https://imgs.search.brave.com/YnXZv_Ywl60pvxz48yHuyN6lG1VDiVex75YAGe84YXU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZjA5MWUxM2My/MTJkMDBkYjdmNTMy/OGZmMzVmYzdmMDc3/MWYzMDM2M2RmODU1/YjNlYjQzNDAwYjYw/YTI4NTg1OS9kbC5h/Y20ub3JnLw', description='Recently, a dizzying number of “X-former” models have been proposed—<strong>Reformer, Linformer, Performer, Longformer</strong>, to name a few—which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency.'), TextChunk(text='.  \\n  \\n4. **Handling Long Sequences**: Innovations like Transformer-XL and Reformer have been developed to handle long sequences more efficiently. Future models are expected to further improve memory mechanisms, sparse attention, and recurrence to process large-scale sequential data more effectively', type='text'), ToolReferenceChunk(tool='web_search', title='The Future of Transformers: Emerging Trends and Research Directions | by Hassaan Idrees | Medium', type='tool_reference', url='https://medium.com/@hassaanidrees7/the-future-of-transformers-emerging-trends-and-research-directions-d3eddce993f6', favicon='https://imgs.search.brave.com/4R4hFITz_F_be0roUiWbTZKhsywr3fnLTMTkFL5HFow/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOTZhYmQ1N2Q4/NDg4ZDcyODIyMDZi/MzFmOWNhNjE3Y2E4/Y2YzMThjNjljNDIx/ZjllZmNhYTcwODhl/YTcwNDEzYy9tZWRp/dW0uY29tLw', description='<strong>Pre-training with Diverse Data</strong>: Training on more diverse and balanced datasets will help ensure models perform well across different languages and cultures, reducing bias and improving global applicability. The combination of Transformers with reinforcement learning (RL) is an exciting area ...'), TextChunk(text='.  \\n  \\n5. **Multilingual Capabilities**: There is a growing focus on developing multilingual transformer models. Models like mBERT (Multilingual BERT) and XLM-R (Cross-lingual Language Model) are examples of early multilingual models. Future advancements aim to improve cross-lingual transfer learning, particularly for low-resource languages where labeled data is scarce', type='text'), ToolReferenceChunk(tool='web_search', title='The Future of Transformers: Emerging Trends and Research Directions | by Hassaan Idrees | Medium', type='tool_reference', url='https://medium.com/@hassaanidrees7/the-future-of-transformers-emerging-trends-and-research-directions-d3eddce993f6', favicon='https://imgs.search.brave.com/4R4hFITz_F_be0roUiWbTZKhsywr3fnLTMTkFL5HFow/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOTZhYmQ1N2Q4/NDg4ZDcyODIyMDZi/MzFmOWNhNjE3Y2E4/Y2YzMThjNjljNDIx/ZjllZmNhYTcwODhl/YTcwNDEzYy9tZWRp/dW0uY29tLw', description='<strong>Pre-training with Diverse Data</strong>: Training on more diverse and balanced datasets will help ensure models perform well across different languages and cultures, reducing bias and improving global applicability. The combination of Transformers with reinforcement learning (RL) is an exciting area ...'), TextChunk(text='.  \\n  \\n6. **Integration with Reinforcement Learning**: Combining transformers with reinforcement learning (RL) is an exciting area of research. This integration aims to enhance the decision-making capabilities of transformer models, making them more adaptable and efficient in various applications', type='text'), ToolReferenceChunk(tool='web_search', title='The Future of Transformers: Emerging Trends and Research Directions | by Hassaan Idrees | Medium', type='tool_reference', url='https://medium.com/@hassaanidrees7/the-future-of-transformers-emerging-trends-and-research-directions-d3eddce993f6', favicon='https://imgs.search.brave.com/4R4hFITz_F_be0roUiWbTZKhsywr3fnLTMTkFL5HFow/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOTZhYmQ1N2Q4/NDg4ZDcyODIyMDZi/MzFmOWNhNjE3Y2E4/Y2YzMThjNjljNDIx/ZjllZmNhYTcwODhl/YTcwNDEzYy9tZWRp/dW0uY29tLw', description='<strong>Pre-training with Diverse Data</strong>: Training on more diverse and balanced datasets will help ensure models perform well across different languages and cultures, reducing bias and improving global applicability. The combination of Transformers with reinforcement learning (RL) is an exciting area ...'), TextChunk(text='.  \\n  \\nThese improvements reflect the ongoing efforts to optimize transformer models for better performance, efficiency, and applicability across different domains and languages.', type='text')]\n"
     ]
    }
   ],
   "source": [
    "print(search_response.outputs[1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "440cd694",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_text=\" \"\n",
    "\n",
    "for result in search_response.outputs[1].content:\n",
    "    if hasattr(result, \"text\"):\n",
    "        markdown_text+=result.text\n",
    "        if hasattr(result,\"tool\"):\n",
    "            markdown_text+=f\"([{result.title}]({result.url}))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "78e96f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " The latest improvements in transformer architecture in 2025 include several key advancements aimed at enhancing performance, efficiency, and scalability. Here are some of the notable developments:  \n",
       "  \n",
       "1. **Pre-normalization**: Modern transformer architectures have shifted from post-normalization to pre-normalization. In pre-normalization, normalization layers are applied before the self-attention and feedforward components. This change improves gradient flow, especially in deep networks, leading to better training stability.  \n",
       "  \n",
       "2. **Rotary Position Embeddings (RoPE)**: Rotary position embeddings have replaced the traditional sinusoidal positional encodings. RoPE encodes relative positional information more effectively, integrating it naturally into the self-attention mechanism. This allows the model to generalize better to sequences longer than those seen during training, improving performance on tasks involving long-context inputs.  \n",
       "  \n",
       "3. **Efficiency Improvements**: Various \"X-former\" models, such as Reformer, Linformer, Performer, and Longformer, have been introduced to improve computational and memory efficiency. These models address the quadratic complexity issue of the self-attention mechanism and reduce computation costs through techniques like pooling and sparsity.  \n",
       "  \n",
       "4. **Handling Long Sequences**: Innovations like Transformer-XL and Reformer have been developed to handle long sequences more efficiently. Future models are expected to further improve memory mechanisms, sparse attention, and recurrence to process large-scale sequential data more effectively.  \n",
       "  \n",
       "5. **Multilingual Capabilities**: There is a growing focus on developing multilingual transformer models. Models like mBERT (Multilingual BERT) and XLM-R (Cross-lingual Language Model) are examples of early multilingual models. Future advancements aim to improve cross-lingual transfer learning, particularly for low-resource languages where labeled data is scarce.  \n",
       "  \n",
       "6. **Integration with Reinforcement Learning**: Combining transformers with reinforcement learning (RL) is an exciting area of research. This integration aims to enhance the decision-making capabilities of transformer models, making them more adaptable and efficient in various applications.  \n",
       "  \n",
       "These improvements reflect the ongoing efforts to optimize transformer models for better performance, efficiency, and applicability across different domains and languages."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(markdown_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48c4fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
